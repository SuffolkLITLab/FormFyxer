{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brycew/Developer/LITLab/litlab_venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from formfyxer import lit_explorer\n",
    "import pingouin as pg\n",
    "import requests\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Full name', 'form_url',\n",
      "       'Using your own judgment, what do you think that the minimum reading grade level for this document is?\\n',\n",
      "       'What is your best guess as to how long it would take a typical litigant to complete this form accurately?\\n',\n",
      "       'The fields are organized so that similar topics are adjacent to each other',\n",
      "       'The number of fields on each page is about right',\n",
      "       'Most litigants can read and understand the prompts and instructions on this form',\n",
      "       'Most litigants can accurately fill in this form without help from an attorney',\n",
      "       'Most litigants can provide a complete answer to the questions on this form',\n",
      "       'Most litigants can provide an answer to the questions on this form without having to ask a third party',\n",
      "       'Is there a page that is radically different from the other pages on the form such that your answers above do not apply to each page? Which page and why?',\n",
      "       'Does this form require the litigant to do math?\\n',\n",
      "       'Does this form require the litigant to describe, recall, or relive a traumatic experience?\\n',\n",
      "       'Does this form require the litigant to disclose sensitive or personal information, other than their name and contact information? For example, a Social Security Number, bank account number, etc.\\n',\n",
      "       'ratings',\n",
      "       'Is the complexity of this form about right given its topic and the remedy it can provide a litigant?\\n',\n",
      "       'good_column'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "all_expert = pd.read_excel(\"RateMyPDF Individual Form Expert Benchmarks(1-176).xlsx\")\n",
    "name_column = \"Full name\"\n",
    "good_column=\"good_column\"\n",
    "rating_column = 'ratings'\n",
    "all_expert = all_expert.rename(columns={\n",
    "  \"What form are you scoring? Please include the full URL, like: https://courtformsonline.org/forms/6e420f1b3575cfd8ef94b71977da9e38252e3395a78439709c760de4.pdf\\n\": \"form_url\",\n",
    "  \"From 1-5, with 1 being the easiest and 5 being the hardest, how complex do you think this form is?\\n \": rating_column,\n",
    "  \"From 1-5 stars, with 5 being the best, how good a form do you think this is? Use any criteria that make sense to you.\\n\": good_column,\n",
    "})\n",
    "form_column = 'form_name'\n",
    "all_expert[form_column] = all_expert['form_url'].apply(lambda y: y.split(\"/\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['form_url', 'ratings', 'Full name', 'good_column', 'form_name',\n",
      "       'reviewer_mean', 'reviewer_stddev', 'z_score', 'reviewer_good_mean',\n",
      "       'reviewer_good_stddev', 'z_good_score'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3691754/2652558115.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"form_name\"] = df[form_column].apply(lambda y: y.split(\"/\")[-1].strip())\n",
      "/tmp/ipykernel_3691754/2652558115.py:21: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df.groupby(\"form_name\")\n"
     ]
    }
   ],
   "source": [
    "df = all_expert[[form_column, rating_column, name_column, good_column]]\n",
    "def normalize_df(df, group_by_column, normalize_column, output_column):\n",
    "  mean_and_stddev = (\n",
    "      df.groupby(group_by_column)[normalize_column]\n",
    "      .agg([\"mean\", \"std\"])\n",
    "      .rename(columns={\"mean\": f\"{normalize_column}_mean\", \"std\": f\"{normalize_column}_stddev\"})\n",
    "      .reset_index()\n",
    "  )\n",
    "  df = pd.merge(df, mean_and_stddev, on=group_by_column)\n",
    "  df[output_column] = (df[normalize_column] - df[f\"{normalize_column}_mean\"]) / df[f\"{normalize_column}_stddev\"]\n",
    "  \n",
    "mean_and_stddev = (\n",
    "    df.groupby(name_column)[rating_column]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .rename(columns={\"mean\": \"reviewer_mean\", \"std\": \"reviewer_stddev\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df = pd.merge(df, mean_and_stddev, on=name_column)\n",
    "df[\"z_score\"] = (df[rating_column] - df[\"reviewer_mean\"]) / df[\"reviewer_stddev\"]\n",
    "\n",
    "mean_and_stddev_good = (\n",
    "    df.groupby(name_column)[good_column]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .rename(columns={\"mean\": \"reviewer_good_mean\", \"std\": \"reviewer_good_stddev\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df = pd.merge(df, mean_and_stddev_good, on=name_column)\n",
    "df[\"z_good_score\"] = (df[good_column] - df[\"reviewer_good_mean\"]) / df[\"reviewer_good_stddev\"]\n",
    "sorted_experts = (\n",
    "    df.groupby(\"form_name\")\n",
    "    .mean()\n",
    "    .sort_values(by=\"z_score\", ascending=True)\n",
    "    .reset_index()\n",
    ")\n",
    "print(df.keys())\n",
    "# print(pd.pivot_table(df[[name_column, 'form_name', rating_column]], index=name_column, columns='form_name').T)\n",
    "# print(df.groupby('form_name').)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n"
     ]
    }
   ],
   "source": [
    "def calc_score(fname):\n",
    "    # full_url = \"https://courtformsonline.org/forms/\" + fname\n",
    "    # local_filename = \"/tmp/\" + fname\n",
    "    # with requests.get(full_url, stream=True) as r:\n",
    "    #  with open(local_filename, 'wb') as f:\n",
    "    #    shutil.copyfileobj(r.raw, f)\n",
    "    # The already downloaded and re-formated field PDFs, the ones that the stats are from in the paper.\n",
    "    local_filename = \"../../../tmp3/\" + fname\n",
    "    stats = lit_explorer.parse_form(local_filename)\n",
    "    return lit_explorer.form_complexity(stats)\n",
    "\n",
    "\n",
    "sorted_experts[\"our_complexity_score\"] = sorted_experts[\"form_name\"].apply(calc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_experts[\"z_complexity_score\"] = (\n",
    "    sorted_experts[\"our_complexity_score\"]\n",
    "    - np.mean(sorted_experts[\"our_complexity_score\"])\n",
    ") / np.std(sorted_experts[\"our_complexity_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Type              Description       ICC         F  df1  df2      pval  \\\n",
      "0   ICC1   Single raters absolute  0.318245  3.800813    4   25  0.015098   \n",
      "1   ICC2     Single random raters  0.320442  3.912134    4   20  0.016651   \n",
      "2   ICC3      Single fixed raters  0.326761  3.912134    4   20  0.016651   \n",
      "3  ICC1k  Average raters absolute  0.736898  3.800813    4   25  0.015098   \n",
      "4  ICC2k    Average random raters  0.738854  3.912134    4   20  0.016651   \n",
      "5  ICC3k     Average fixed raters  0.744385  3.912134    4   20  0.016651   \n",
      "\n",
      "          CI95%  \n",
      "0  [0.02, 0.84]  \n",
      "1  [0.03, 0.84]  \n",
      "2  [0.02, 0.84]  \n",
      "3  [0.12, 0.97]  \n",
      "4  [0.14, 0.97]  \n",
      "5   [0.1, 0.97]  \n",
      "    Type              Description       ICC         F  df1  df2      pval  \\\n",
      "0   ICC1   Single raters absolute  0.402108  5.707792    4   30  0.001540   \n",
      "1   ICC2     Single random raters  0.404255  5.959322    4   24  0.001775   \n",
      "2   ICC3      Single fixed raters  0.414683  5.959322    4   24  0.001775   \n",
      "3  ICC1k  Average raters absolute  0.824801  5.707792    4   30  0.001540   \n",
      "4  ICC2k    Average random raters  0.826087  5.959322    4   24  0.001775   \n",
      "5  ICC3k     Average fixed raters  0.832196  5.959322    4   24  0.001775   \n",
      "\n",
      "          CI95%  \n",
      "0   [0.1, 0.87]  \n",
      "1   [0.1, 0.87]  \n",
      "2   [0.1, 0.88]  \n",
      "3  [0.43, 0.98]  \n",
      "4  [0.44, 0.98]  \n",
      "5  [0.43, 0.98]  \n",
      "    Type              Description       ICC         F  df1  df2      pval  \\\n",
      "0   ICC1   Single raters absolute  0.229057  3.079787    4   30  0.030793   \n",
      "1   ICC2     Single random raters  0.232653  3.216667    4   24  0.030056   \n",
      "2   ICC3      Single fixed raters  0.240506  3.216667    4   24  0.030056   \n",
      "3  ICC1k  Average raters absolute  0.675302  3.079787    4   30  0.030793   \n",
      "4  ICC2k    Average random raters  0.679727  3.216667    4   24  0.030056   \n",
      "5  ICC3k     Average fixed raters  0.689119  3.216667    4   24  0.030056   \n",
      "\n",
      "           CI95%  \n",
      "0  [-0.01, 0.78]  \n",
      "1   [-0.0, 0.78]  \n",
      "2  [-0.01, 0.79]  \n",
      "3  [-0.06, 0.96]  \n",
      "4  [-0.01, 0.96]  \n",
      "5  [-0.05, 0.96]  \n"
     ]
    }
   ],
   "source": [
    "sorted_experts_copy = sorted_experts.copy()\n",
    "sorted_experts_copy[name_column] = \"Algo\"\n",
    "sorted_experts_copy[\"z_score\"] = sorted_experts_copy[\"z_complexity_score\"]\n",
    "df_algo = df.copy()\n",
    "random_df = pd.DataFrame()\n",
    "import random\n",
    "random_df['z_score'] = (np.array(random.sample(range(0, 1000), len(sorted_experts_copy))) - 500) / 250\n",
    "random_df[name_column] = 'random'\n",
    "random_df['form_name'] = sorted_experts['form_name']\n",
    "df_algo_grader = pd.concat([df_algo, sorted_experts_copy])\n",
    "df_random_grader = pd.concat([df_algo, random_df])\n",
    "\n",
    "bins = [-2, -1, 0, 1, 2, 3]\n",
    "df[\"z_binned\"] = np.digitize(df[\"z_score\"], bins=bins)\n",
    "df_algo_grader[\"z_binned\"] = np.digitize(df_algo_grader[\"z_score\"], bins=bins)\n",
    "df_random_grader[\"z_binned\"] = np.digitize(df_random_grader[\"z_score\"], bins=bins)\n",
    "results_complex = pg.intraclass_corr(data=df, targets='form_name', raters=name_column, ratings=\"z_binned\", nan_policy='omit')\n",
    "results_z_complex_with_us = pg.intraclass_corr(data=df_algo_grader, targets='form_name', raters=name_column, ratings=\"z_binned\", nan_policy='omit')\n",
    "results_z_complex_with_random = pg.intraclass_corr(data=df_random_grader, targets='form_name', raters=name_column, ratings=\"z_binned\", nan_policy='omit')\n",
    "\n",
    "#print(df_algo_grader.groupby(name_column).mean())\n",
    "print(results_complex)\n",
    "print(results_z_complex_with_us)\n",
    "print(results_z_complex_with_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions are used to count the number of adjacent swaps needed\n",
    "# to change one ordering into another. It was written by Shivam Gupta\n",
    "# See https://www.geeksforgeeks.org/number-swaps-sort-adjacent-swapping-allowed/\n",
    "# -----------------------------------------------------\n",
    "# python 3 program to count number of swaps required\n",
    "# to sort an array when only swapping of adjacent\n",
    "# elements is allowed.\n",
    "# include <bits/stdc++.h>\n",
    "\n",
    "\n",
    "# This function merges two sorted arrays and returns inversion count in the arrays.*/\n",
    "def merge(arr, temp, left, mid, right):\n",
    "    inv_count = 0\n",
    "\n",
    "    i = left  # i is index for left subarray*/\n",
    "    j = mid  # i is index for right subarray*/\n",
    "    k = left  # i is index for resultant merged subarray*/\n",
    "    while (i <= mid - 1) and (j <= right):\n",
    "        if arr[i] <= arr[j]:\n",
    "            temp[k] = arr[i]\n",
    "            k += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            temp[k] = arr[j]\n",
    "            k += 1\n",
    "            j += 1\n",
    "\n",
    "            # this is tricky -- see above explanation/\n",
    "            # diagram for merge()*/\n",
    "            inv_count = inv_count + (mid - i)\n",
    "\n",
    "    # Copy the remaining elements of left subarray\n",
    "    # (if there are any) to temp*/\n",
    "    while i <= mid - 1:\n",
    "        temp[k] = arr[i]\n",
    "        k += 1\n",
    "        i += 1\n",
    "\n",
    "    # Copy the remaining elements of right subarray\n",
    "    # (if there are any) to temp*/\n",
    "    while j <= right:\n",
    "        temp[k] = arr[j]\n",
    "        k += 1\n",
    "        j += 1\n",
    "\n",
    "    # Copy back the merged elements to original array*/\n",
    "    for i in range(left, right + 1, 1):\n",
    "        arr[i] = temp[i]\n",
    "\n",
    "    return inv_count\n",
    "\n",
    "\n",
    "# An auxiliary recursive function that sorts the input\n",
    "# array and returns the number of inversions in the\n",
    "# array. */\n",
    "def _mergeSort(arr, temp, left, right):\n",
    "    inv_count = 0\n",
    "    if right > left:\n",
    "        # Divide the array into two parts and call\n",
    "        # _mergeSortAndCountInv()\n",
    "        # for each of the parts */\n",
    "        mid = int((right + left) / 2)\n",
    "\n",
    "        # Inversion count will be sum of inversions in\n",
    "        # left-part, right-part and number of inversions\n",
    "        # in merging */\n",
    "        inv_count = _mergeSort(arr, temp, left, mid)\n",
    "        inv_count += _mergeSort(arr, temp, mid + 1, right)\n",
    "\n",
    "        # Merge the two parts*/\n",
    "        inv_count += merge(arr, temp, left, mid + 1, right)\n",
    "\n",
    "    return inv_count\n",
    "\n",
    "\n",
    "# This function sorts the input array and returns the\n",
    "# number of inversions in the array */\n",
    "def countSwaps(arr):\n",
    "    n = len(arr)\n",
    "    temp = [0 for i in range(n)]\n",
    "    return _mergeSort(arr, temp, 0, n - 1)  # Time to get random sorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sort count: 388.256\n",
      "Machine sort count: 306.0\n"
     ]
    }
   ],
   "source": [
    "# Time to get sorts\n",
    "def get_avg_sort_count(df, final_order_col, sort_by, runs=1):\n",
    "    swap_counts = []\n",
    "    for i in range(runs):\n",
    "        if sort_by == \"random\":\n",
    "            array = df[final_order_col].sample(frac=1).values.copy()\n",
    "        else:\n",
    "            df = df.sample(frac=1).sort_values(by=sort_by, ascending=True)\n",
    "            array = df[final_order_col].values.copy()\n",
    "        swap_counts.append(countSwaps(array))\n",
    "    return np.array(swap_counts).mean()\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Random sort count: {get_avg_sort_count(sorted_experts, 'z_score', 'random', runs=5000)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Machine sort count: {get_avg_sort_count(sorted_experts, 'z_score', 'z_complexity_score', runs=5000)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy_cohen_kappa: 0.6160267111853088\n"
     ]
    }
   ],
   "source": [
    "# Time to get cohen's kappa\n",
    "# put both ours and theirs back into bins.\n",
    "bins = [-3, -2, -1, 0, 1, 2, 3]\n",
    "expert_bins = np.digitize(sorted_experts[\"z_score\"], bins=bins)\n",
    "algo_bins = np.digitize(sorted_experts[\"z_complexity_score\"], bins=bins)\n",
    "print(\n",
    "    f'scipy_cohen_kappa: {cohen_kappa_score(expert_bins, algo_bins, weights=\"quadratic\")}'\n",
    ")\n",
    "# print(stats.ttest_rel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Type              Description       ICC         F  df1  df2      pval  \\\n",
      "0   ICC1   Single raters absolute  0.318245  3.800813    4   25  0.015098   \n",
      "1   ICC2     Single random raters  0.320442  3.912134    4   20  0.016651   \n",
      "2   ICC3      Single fixed raters  0.326761  3.912134    4   20  0.016651   \n",
      "3  ICC1k  Average raters absolute  0.736898  3.800813    4   25  0.015098   \n",
      "4  ICC2k    Average random raters  0.738854  3.912134    4   20  0.016651   \n",
      "5  ICC3k     Average fixed raters  0.744385  3.912134    4   20  0.016651   \n",
      "\n",
      "          CI95%  \n",
      "0  [0.02, 0.84]  \n",
      "1  [0.03, 0.84]  \n",
      "2  [0.02, 0.84]  \n",
      "3  [0.12, 0.97]  \n",
      "4  [0.14, 0.97]  \n",
      "5   [0.1, 0.97]  \n",
      "    Type              Description       ICC         F  df1  df2      pval  \\\n",
      "0   ICC1   Single raters absolute  0.402108  5.707792    4   30  0.001540   \n",
      "1   ICC2     Single random raters  0.404255  5.959322    4   24  0.001775   \n",
      "2   ICC3      Single fixed raters  0.414683  5.959322    4   24  0.001775   \n",
      "3  ICC1k  Average raters absolute  0.824801  5.707792    4   30  0.001540   \n",
      "4  ICC2k    Average random raters  0.826087  5.959322    4   24  0.001775   \n",
      "5  ICC3k     Average fixed raters  0.832196  5.959322    4   24  0.001775   \n",
      "\n",
      "          CI95%  \n",
      "0   [0.1, 0.87]  \n",
      "1   [0.1, 0.87]  \n",
      "2   [0.1, 0.88]  \n",
      "3  [0.43, 0.98]  \n",
      "4  [0.44, 0.98]  \n",
      "5  [0.43, 0.98]  \n",
      "    Type              Description       ICC         F  df1  df2      pval  \\\n",
      "0   ICC1   Single raters absolute  0.600373  4.004662   39   40  0.000014   \n",
      "1   ICC2     Single random raters  0.598690  3.922374   39   39  0.000021   \n",
      "2   ICC3      Single fixed raters  0.593692  3.922374   39   39  0.000021   \n",
      "3  ICC1k  Average raters absolute  0.750291  4.004662   39   40  0.000014   \n",
      "4  ICC2k    Average random raters  0.748976  3.922374   39   39  0.000021   \n",
      "5  ICC3k     Average fixed raters  0.745052  3.922374   39   39  0.000021   \n",
      "\n",
      "          CI95%  \n",
      "0  [0.36, 0.77]  \n",
      "1  [0.35, 0.77]  \n",
      "2  [0.35, 0.76]  \n",
      "3  [0.53, 0.87]  \n",
      "4  [0.52, 0.87]  \n",
      "5  [0.52, 0.87]  \n"
     ]
    }
   ],
   "source": [
    "bins = [-2, -1, 0, 1, 2, 3]\n",
    "expert_bins = np.digitize(sorted_experts[\"z_score\"], bins=bins)\n",
    "algo_bins = np.digitize(sorted_experts[\"z_complexity_score\"], bins=bins)\n",
    "\n",
    "new_df = pd.DataFrame([], columns=[\"reviewer\", \"score\"])\n",
    "new_df_algo = new_df.copy()\n",
    "new_df_algo[\"score\"] = algo_bins\n",
    "new_df_algo[\"reviewer\"] = \"algo\"\n",
    "new_df_algo[\"idx\"] = new_df_algo.index\n",
    "new_df_expert = new_df.copy()\n",
    "new_df_expert[\"score\"] = expert_bins\n",
    "new_df_expert[\"reviewer\"] = \"expert\"\n",
    "new_df_expert[\"idx\"] = new_df_expert.index\n",
    "new_df = pd.concat([new_df_algo, new_df_expert])\n",
    "\n",
    "df[\"z_binned\"] = np.digitize(df[\"z_score\"], bins=bins)\n",
    "results_complex = pg.intraclass_corr(data=df, targets='form_name', raters=name_column, ratings=rating_column, nan_policy='omit')\n",
    "results_good = pg.intraclass_corr(data=df, targets='form_name', raters=name_column, ratings=good_column, nan_policy='omit')\n",
    "results_z_good = pg.intraclass_corr(data=df, targets='form_name', raters=name_column, ratings='z_good_score', nan_policy='omit')\n",
    "results_z_complex = pg.intraclass_corr(data=df, targets='form_name', raters=name_column, ratings=\"z_binned\", nan_policy='omit')\n",
    "\n",
    "results_complex = pg.intraclass_corr(data=new_df, targets=\"idx\", raters=\"reviewer\", ratings=\"score\")\n",
    "\n",
    "print(results_z_complex)\n",
    "print(results_z_complex_with_us)\n",
    "print(results_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3691754/3204406776.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df.groupby(name_column).mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>good_column</th>\n",
       "      <th>reviewer_mean</th>\n",
       "      <th>reviewer_stddev</th>\n",
       "      <th>z_score</th>\n",
       "      <th>z_binned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Full name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reviewer 1</th>\n",
       "      <td>3.200000</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.484234</td>\n",
       "      <td>-3.441691e-16</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 2</th>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.517241</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.061337</td>\n",
       "      <td>-1.480297e-16</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 3</th>\n",
       "      <td>1.742857</td>\n",
       "      <td>4.257143</td>\n",
       "      <td>1.742857</td>\n",
       "      <td>1.010034</td>\n",
       "      <td>-2.537653e-17</td>\n",
       "      <td>2.742857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 4</th>\n",
       "      <td>2.680000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>1.107550</td>\n",
       "      <td>-1.643130e-16</td>\n",
       "      <td>2.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 5</th>\n",
       "      <td>1.914286</td>\n",
       "      <td>3.342857</td>\n",
       "      <td>1.914286</td>\n",
       "      <td>0.981338</td>\n",
       "      <td>-7.612958e-17</td>\n",
       "      <td>2.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 6</th>\n",
       "      <td>1.850000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.136708</td>\n",
       "      <td>-5.551115e-17</td>\n",
       "      <td>2.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ratings  good_column  reviewer_mean  reviewer_stddev  \\\n",
       "Full name                                                           \n",
       "Reviewer 1  3.200000     2.066667       3.200000         0.484234   \n",
       "Reviewer 2  2.333333     3.517241       2.333333         1.061337   \n",
       "Reviewer 3  1.742857     4.257143       1.742857         1.010034   \n",
       "Reviewer 4  2.680000     3.200000       2.680000         1.107550   \n",
       "Reviewer 5  1.914286     3.342857       1.914286         0.981338   \n",
       "Reviewer 6  1.850000     3.200000       1.850000         1.136708   \n",
       "\n",
       "                 z_score  z_binned  \n",
       "Full name                           \n",
       "Reviewer 1 -3.441691e-16  2.400000  \n",
       "Reviewer 2 -1.480297e-16  2.333333  \n",
       "Reviewer 3 -2.537653e-17  2.742857  \n",
       "Reviewer 4 -1.643130e-16  2.680000  \n",
       "Reviewer 5 -7.612958e-17  2.914286  \n",
       "Reviewer 6 -5.551115e-17  2.750000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(name_column).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litlab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
