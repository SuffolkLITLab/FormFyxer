{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RateMyPdf: Expert rankings analysis\n",
    "\n",
    "This notebook walks you through our process for analyzing our complexity score compared to expert rankings of court forms.\n",
    "\n",
    "Before you run this notebook, be sure that you install the FormFyxer (see [this package's README.md](../README.md) for full instructions), and then run `pip install -r analysis_requirements.txt`, so you have all the necessary dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from formfyxer import lit_explorer, pdf_wrangling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the reviewer data. There are more columns in the excel spreadsheet, but the important ones for our analysis are:\n",
    "* the name of the reviewer (anonymized as Reviewer 1, 2, etc.)\n",
    "* the full URL of the form. We just look at the ending file for uniqueness\n",
    "* the reviewer's rating of how complex the form is. The full text prompt is shown below, but we'll call this \"ratings\" or \"rating_column\"\n",
    "* the reviewer's rating of how good the form. The full text is also shown below, but we'll call this \"goodness\", or \"good_column\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_expert = pd.read_excel(\"RateMyPDF Individual Form Expert Benchmarks(1-176).xlsx\")\n",
    "name_column = \"Full name\"\n",
    "good_column=\"good_column\"\n",
    "rating_column = 'ratings'\n",
    "all_expert = all_expert.rename(columns={\n",
    "  \"What form are you scoring? Please include the full URL, like: https://courtformsonline.org/forms/6e420f1b3575cfd8ef94b71977da9e38252e3395a78439709c760de4.pdf\\n\": \"form_url\",\n",
    "  \"From 1-5, with 1 being the easiest and 5 being the hardest, how complex do you think this form is?\\n \": rating_column,\n",
    "  \"From 1-5 stars, with 5 being the best, how good a form do you think this is? Use any criteria that make sense to you.\\n\": good_column,\n",
    "})\n",
    "form_column = 'form_name'\n",
    "all_expert[form_column] = all_expert['form_url'].apply(lambda y: y.split(\"/\")[-1].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data. Let's look at some simple stats, like the mean and standard deviation of scores that each reviewer gave. Each reviewer was assigned between 20 and 35 forms to review, and each form got between 3 and 6 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">ratings</th>\n",
       "      <th colspan=\"3\" halign=\"left\">good_column</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Full name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reviewer 1</th>\n",
       "      <td>30</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.484234</td>\n",
       "      <td>30</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>0.253708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 2</th>\n",
       "      <td>30</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.061337</td>\n",
       "      <td>29</td>\n",
       "      <td>3.517241</td>\n",
       "      <td>0.737791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 3</th>\n",
       "      <td>35</td>\n",
       "      <td>1.742857</td>\n",
       "      <td>1.010034</td>\n",
       "      <td>35</td>\n",
       "      <td>4.257143</td>\n",
       "      <td>1.093910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 4</th>\n",
       "      <td>25</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>1.107550</td>\n",
       "      <td>25</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 5</th>\n",
       "      <td>35</td>\n",
       "      <td>1.914286</td>\n",
       "      <td>0.981338</td>\n",
       "      <td>35</td>\n",
       "      <td>3.342857</td>\n",
       "      <td>0.591253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reviewer 6</th>\n",
       "      <td>20</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.136708</td>\n",
       "      <td>20</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1.281447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ratings                     good_column                    \n",
       "             count      mean       std       count      mean       std\n",
       "Full name                                                             \n",
       "Reviewer 1      30  3.200000  0.484234          30  2.066667  0.253708\n",
       "Reviewer 2      30  2.333333  1.061337          29  3.517241  0.737791\n",
       "Reviewer 3      35  1.742857  1.010034          35  4.257143  1.093910\n",
       "Reviewer 4      25  2.680000  1.107550          25  3.200000  1.000000\n",
       "Reviewer 5      35  1.914286  0.981338          35  3.342857  0.591253\n",
       "Reviewer 6      20  1.850000  1.136708          20  3.200000  1.281447"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_expert[[name_column, rating_column, good_column]].groupby(name_column).agg(['count', 'mean', 'std'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we normalize the rating and goodness over each reviewer. Say that a certain review is judges everything to be easy, meaning they only give 1s and 2s, even though the complexity is from 1 to 5, but another reviewer gives only 4s and 5s. They might agree that certain forms are more complex that others, but their scores won't. \n",
    "\n",
    "As we can see by the data above, each reviewer had a very different mean, in both the ratings and the goodness column. By normalizing the ratings and goodness, we can more accurately compare each reviewer's score to each other's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_expert[[form_column, rating_column, name_column, good_column]]\n",
    "def normalize_df(df, *, group_by, to_normalize, output_col):\n",
    "  mean_col = f\"{to_normalize}_mean\"\n",
    "  stddev_col = f\"{to_normalize}_stddev\"\n",
    "  mean_and_stddev = (\n",
    "      df.groupby(group_by)[to_normalize]\n",
    "      .agg([\"mean\", \"std\"])\n",
    "      .rename(columns={\"mean\": mean_col, \"std\": stddev_col})\n",
    "      .reset_index()\n",
    "  )\n",
    "  df = pd.merge(df, mean_and_stddev, on=group_by)\n",
    "  df[output_col] = (df[to_normalize] - df[mean_col]) / df[stddev_col]\n",
    "  return df.drop([mean_col, stddev_col], axis='columns')\n",
    "\n",
    "df = normalize_df(df, group_by=name_column, to_normalize=rating_column, output_col=\"z_rating\")\n",
    "df = normalize_df(df, group_by=name_column, to_normalize=good_column, output_col=\"z_goodness\")\n",
    "\n",
    "# Since we had 5 categories at the beginning, we will bin the z-scores back into 5 bins;\n",
    "z_score_bins = [-1.5, -0.5, 0.5, 1.5]\n",
    "# z_score_bins = [-2, -1, 0, 1, 2, 3]\n",
    "# np.digitize returns 0 when val < -1.5, so add 1 to everything to shift back to 1 through 5\n",
    "df[\"z_rating_binned\"] = np.digitize(df[\"z_rating\"], bins=z_score_bins) + 1 \n",
    "df[\"z_goodness_binned\"] = np.digitize(df[\"z_goodness\"], bins=z_score_bins) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first statement that we want to test is that \"each expert reviewer correlates to each other\". This shows that looking at multiple expert rankings is useful. To do this, we use [Intraclass correlation coefficient](https://rowannicholls.github.io/python/statistics/agreement/intraclass_correlation.html); it \"assesses the reliability of ratings by comparing the variability of different ratings of the same subject to the total variation across all ratings and all subjects\" (from the [pingouin documentation](https://pingouin-stats.org/build/html/generated/pingouin.intraclass_corr.html)). Since each form was reviewed by a different sub-set of all reviewers, we'll look at the ICC1, or \"single random raters\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ICC    pval\n",
      "Type                \n",
      "ICC1  0.2675  0.0301\n"
     ]
    }
   ],
   "source": [
    "expert_ratings_results = pg.intraclass_corr(data=df, targets=form_column, raters=name_column, ratings=\"z_rating_binned\", nan_policy='omit')\n",
    "print(expert_ratings_results.set_index('Type').loc[[\"ICC1\"], ['ICC', 'pval']].round(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that we considered when creating our complexity score was what should we be measuring? We decided to measure the complexity of a form specifically, instead of whether or not a form was objectively \"good\". We asked each reviewer both how complex they thought each form was, and also how good they thought each form was. We can look at the ICC1 of the goodness rating; it does indicate that there could higher expert agreement complex compared to goodness. However, the p-value is too high to draw any conclusions from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ICC    pval\n",
      "Type                \n",
      "ICC1  0.1554  0.1103\n"
     ]
    }
   ],
   "source": [
    "expert_goodness_results = pg.intraclass_corr(data=df, targets=form_column, raters=name_column, ratings=\"z_goodness_binned\", nan_policy='omit')\n",
    "print(expert_goodness_results.set_index('Type').loc[[\"ICC1\"], ['ICC', 'pval']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3691754/3327341924.py:8: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df.groupby(\"form_name\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n",
      "Detecting Sentences...\n",
      "Starting to find passives...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"/tmp/ratemypdf_analysis\"):\n",
    "    os.mkdir(\"/tmp/ratemypdf_analysis\")\n",
    "\n",
    "if not os.path.exists(\"/tmp/ratemypdf_analysis/labeled\"):\n",
    "    os.mkdir(\"/tmp/ratemypdf_analysis/labeled\")\n",
    "\n",
    "mean_per_form = (\n",
    "    df.groupby(\"form_name\")\n",
    "    .mean()\n",
    "    .sort_values(by=\"z_rating\", ascending=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "def calc_score(fname):\n",
    "    \"\"\"Calculates our complexity score for the same forms that the experts rated.\n",
    "    If not already downloaded and analysized, will be downloaded and labeled\"\"\"\n",
    "    local_filename = \"/tmp/ratemypdf_analysis/labeled/\" + fname\n",
    "    if not os.path.exists(local_filename):\n",
    "        full_url = \"https://courtformsonline.org/forms/\" + fname\n",
    "        download_loc = \"/tmp/ratemypdf_analysis/\" + fname\n",
    "        with requests.get(full_url, stream=True) as r:\n",
    "            with open(download_loc, 'wb') as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "        all_fields = [f for f_in_page in pdf_wrangling.get_existing_pdf_fields(download_loc) for f in f_in_page]\n",
    "        if not all_fields:\n",
    "            pdf_wrangling.auto_add_fields(download_loc, local_filename)\n",
    "        else:\n",
    "            pdf_wrangling.auto_rename_fields(download_loc, local_filename)\n",
    "    stats = lit_explorer.parse_form(local_filename)\n",
    "    return lit_explorer.form_complexity(stats)\n",
    "\n",
    "\n",
    "mean_per_form[\"complexity_score\"] = mean_per_form[form_column].apply(calc_score)\n",
    "mean = np.mean(mean_per_form[\"complexity_score\"])\n",
    "stddev = np.std(mean_per_form[\"complexity_score\"])\n",
    "mean_per_form[\"z_complexity_score\"] = (mean_per_form[\"complexity_score\"] - mean) / stddev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the complexity score, we can compare it to the other reviews. We'll do so in a few ways:\n",
    "* treating the algorithm as an expert reviewer, and comparing the ICC to just the expert reviewers\n",
    "  * in addition to this, we'll do the same thing but with random scores as the 7th reviewer, and the mean of all reviewers as the 7th reviewer. Our algorithm should fall between those two.\n",
    "* averaging the scores each reviewer gave to their forms, and treating it as a single \"mega-reviewer\" score. We'll compare this to our algorithm's scores to the forms, and see the ICC there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Experts only:\n",
      "          ICC    pval\n",
      "Type                \n",
      "ICC1  0.2675  0.0301\n",
      "---\n",
      "Algorithm as expert:\n",
      "          ICC    pval\n",
      "Type                \n",
      "ICC1  0.3359  0.0055\n",
      "---\n",
      "Random as expert:\n",
      "          ICC    pval\n",
      "Type                \n",
      "ICC1  0.2211  0.0345\n",
      "---\n",
      "Average as expert:\n",
      "          ICC    pval\n",
      "Type                \n",
      "ICC1  0.2865  0.0128\n"
     ]
    }
   ],
   "source": [
    "algo_scores = mean_per_form.copy(deep=True)\n",
    "algo_scores[name_column] = \"Algo\"\n",
    "algo_scores[\"z_rating\"] = algo_scores[\"z_complexity_score\"]\n",
    "algo_scores['z_rating_binned'] = np.digitize(algo_scores['z_rating'], bins=z_score_bins) + 1\n",
    "df_algo_as_reviewer = pd.concat([df, algo_scores])\n",
    "\n",
    "rand_scores = mean_per_form.copy(deep=True)\n",
    "rand_scores[name_column] = 'random'\n",
    "rand_scores['z_rating'] = np.random.rand(len(rand_scores)) * 5 - 2.5\n",
    "rand_scores['z_rating_binned'] = np.digitize(rand_scores['z_rating'], bins=z_score_bins) + 1\n",
    "df_rand_as_reviewer = pd.concat([df, rand_scores])\n",
    "\n",
    "avg_scores = mean_per_form.copy(deep=True)\n",
    "avg_scores[name_column] = 'avg'\n",
    "avg_scores['z_rating'] = avg_scores['z_rating']\n",
    "avg_scores['z_rating_binned'] = np.digitize(avg_scores['z_rating'], bins=z_score_bins) + 1\n",
    "df_avg_as_reviewer = pd.concat([df, avg_scores])\n",
    "\n",
    "algo_as_expert_results = pg.intraclass_corr(data=df_algo_as_reviewer, targets=form_column, raters=name_column, ratings=\"z_rating_binned\", nan_policy='omit')\n",
    "rand_as_expert_results = pg.intraclass_corr(data=df_rand_as_reviewer, targets=form_column, raters=name_column, ratings=\"z_rating_binned\", nan_policy='omit')\n",
    "avg_as_expert_results = pg.intraclass_corr(data=df_avg_as_reviewer, targets=form_column, raters=name_column, ratings=\"z_rating_binned\", nan_policy='omit')\n",
    "\n",
    "#print(df_algo_grader.groupby(name_column).mean())\n",
    "print(f\"---\\nExperts only:\\n {expert_ratings_results.set_index('Type').loc[['ICC1'], ['ICC', 'pval']].round(4)}\")\n",
    "print(f\"---\\nAlgorithm as expert:\\n {algo_as_expert_results.set_index('Type').loc[['ICC1'], ['ICC', 'pval']].round(4)}\")\n",
    "print(f\"---\\nRandom as expert:\\n {rand_as_expert_results.set_index('Type').loc[['ICC1'], ['ICC', 'pval']].round(4)}\")\n",
    "print(f\"---\\nAverage as expert:\\n {avg_as_expert_results.set_index('Type').loc[['ICC1'], ['ICC', 'pval']].round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Algo and experts as a single unit:\n",
      "          ICC    pval\n",
      "Type                \n",
      "ICC3  0.3151  0.0224\n"
     ]
    }
   ],
   "source": [
    "tmp_df_algo = pd.DataFrame([], columns=[\"reviewer\", \"rating\"])\n",
    "tmp_df_algo[\"rating\"] = mean_per_form[\"z_complexity_score\"]\n",
    "tmp_df_algo[\"reviewer\"] = \"algo\"\n",
    "tmp_df_algo[\"idx\"] = tmp_df_algo.index\n",
    "tmp_df_mega = pd.DataFrame([], columns=[\"reviewer\", \"rating\"])\n",
    "tmp_df_mega[\"rating\"] = mean_per_form[\"z_rating\"]\n",
    "tmp_df_mega[\"reviewer\"] = \"expert\"\n",
    "tmp_df_mega[\"idx\"] = tmp_df_mega.index\n",
    "algo_vs_mega = pd.concat([tmp_df_algo, tmp_df_mega])\n",
    "algo_vs_mega[\"rating_binned\"] = np.digitize(algo_vs_mega[\"rating\"], bins=z_score_bins) + 1\n",
    "\n",
    "algo_vs_mega_results = pg.intraclass_corr(data=algo_vs_mega, targets=\"idx\", raters=\"reviewer\", ratings=\"rating_binned\")\n",
    "\n",
    "print(f\"---\\nAlgo and experts as a single unit:\\n {algo_vs_mega_results.set_index('Type').loc[['ICC3'], ['ICC', 'pval']].round(4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litlab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
